{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967760d5-3216-4592-be63-202dadd64124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b124e8-9a50-4098-ac1a-eee6fe8b6799",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utilities.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "494cffd3-b979-46b2-87da-a78865c9cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_model_over_api(modelname, messages):\n",
    "\n",
    "    client = OpenAI(api_key = \"EMPTY\", base_url = \"http://mlserver.iteratec.de:8000/v1\")\n",
    "\n",
    "    chat_completion = '' \n",
    "    try: \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model = modelname,\n",
    "            messages = messages\n",
    "        )\n",
    "\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(\"The server could not be reached\")\n",
    "        print(e.__cause__)  # an underlying Exception, likely raised within httpx.\n",
    "    except openai.RateLimitError as e:\n",
    "        print(\"A 429 status code was received; we should back off a bit.\")\n",
    "    except openai.APIStatusError as e:\n",
    "        print(\"Another non-200-range status code was received\")\n",
    "        print(e.status_code)\n",
    "        print(e.response)\n",
    "    except openai.BadRequestError as e:\n",
    "        print(e.response)\n",
    "    except openai.InternalServerError as e:\n",
    "        print(e.response)\n",
    "    except openai.UnprocessableEntityError as e:\n",
    "        print(e.reponse)\n",
    "    except openai.NotFoundError as e:\n",
    "        print(e.response)\n",
    "    except openai.PermissionDeniedError as e:\n",
    "        print(e.response)\n",
    "\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "# prompt_model_over_api(\"casperhansen/mixtral-instruct-awq\", [{\"role\": \"user\", \"content\": 'system_message'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecf5ef3d-e0e3-434a-ad01-03327f681cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results():\n",
    "    timestamp = str(int(time.time()))\n",
    "\n",
    "    # create the file name\n",
    "    if 'None' in prompt_dropdown.value:\n",
    "        prompt_value = 'NoContext'\n",
    "    else:\n",
    "        prompt_value = 'SpecificContext'\n",
    "\n",
    "    if minimal_target_value_input.value:\n",
    "        target = f'min_target_strength_{minimal_target_value_input.value}'\n",
    "    else:\n",
    "        target = 'min_target_strength_not_specified'\n",
    "\n",
    "    if model_dropdown.value == 'casperhansen/mixtral-instruct-awq':\n",
    "        model_value = 'mixtral-instruct-awq'\n",
    "    else:\n",
    "        model_value = model_dropdown.value\n",
    "\n",
    "    filename = f\"../results/LLM/{model_value}_{prompt_value}_prompt_experiment_{experiment+1}_{target}_Dev_Cycles_{number_development_input.value}_test_time_{test_time_dropdown.value}_{timestamp}_{prompt_dropdown.value}.csv\"\n",
    "       \n",
    "        # open the file in write mode\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # write the headers\n",
    "        writer.writerow([\"Formulation\", \"Compressive Strength\"])\n",
    "\n",
    "        # iterate over the training data\n",
    "        for data in training_data:\n",
    "            try:\n",
    "                # parse the data to extract formulation and compressive strength\n",
    "                formulation, strength_str = data.split(\" resulted in a strength of \")\n",
    "                strength = float(strength_str.split(\" \")[0])  # convert string to float\n",
    "                writer.writerow([formulation, strength])\n",
    "            except ValueError as e:\n",
    "                print(f\"Failed to parse data: {data}\")\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "        print(f\"Data for experiment {experiment+1} successfully saved to {filename}. \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87aa7b10-bbab-4315-9e7a-9eb61d31a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_model_with_extended_test_time(modelname, training_data, messages_inverse_design):\n",
    "    predictions = []\n",
    "    formulations = []\n",
    "    unique_formulations = []\n",
    "    retry_count = 0\n",
    "    max_retries = 10\n",
    "\n",
    "    # Add the new user prompt to ask for three unique formulations, if no previous conversation history, last message is from and just appending another user message causes api error\n",
    "    # If last message from user, append additional message to content else append entirely new message\n",
    "    if messages_inverse_design[-1][\"role\"] == \"user\":     \n",
    "        messages_inverse_design[-1][\"content\"] += (\"\\n Output 3 three completly new and untested formulations with a extremly high expected compressive strength , each of the three must adhere to the given structure and parameter limits!.\")\n",
    "    else:\n",
    "        messages_inverse_design.append({\"role\": \"user\", \"content\": \"Output 3 three completly new and untested formulations with a extremly high expected compressive strength , each of the three must adhere to the given structure and parameter limits!.\"})\n",
    "    \n",
    "\n",
    "\n",
    "    while not predictions and retry_count < max_retries:        \n",
    "        # Make API call\n",
    "        model_response_inverse_design = prompt_model_over_api(model_dropdown.value, messages_inverse_design)\n",
    "        # Extract the three formulations from the model response\n",
    "        response_lines = model_response_inverse_design.split('\\n')\n",
    "        # Define a regular expression pattern to capture the relevant information\n",
    "        pattern = re.compile(r'Powderkg\\s*=\\s*(\\d+),\\s*wc\\s*=\\s*(\\d+\\.\\d+),\\s*materials\\s*=\\s*(\\d+\\.\\d+/\\d+\\.\\d+),\\s*curing\\s*=\\s*(\\w+)', re.IGNORECASE)\n",
    "        for line in response_lines:\n",
    "            if line.strip():\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    formulations.append(line)\n",
    "\n",
    "        # formulations = [line for line in response_lines if line.startswith(\"The formulation is\")]\n",
    "        if training_data:\n",
    "            training_formulations = extract_formulations_from_training_data(training_data)\n",
    "            print(f'Current training formulations :\\n {training_formulations}\\n')\n",
    "            unique_formulations = [f for f in formulations if f not in training_formulations]\n",
    "            print(f'{len(unique_formulations)} unique formulations were found by model during inverse design...')\n",
    "            if len(unique_formulations) == 0:\n",
    "                print(f'no new formulations extracted, {formulations} already in training data')\n",
    "        if not training_data:\n",
    "            unique_formulations = formulations\n",
    "        print(f'These are the three novel formulations: \\n{unique_formulations}\\n')\n",
    "        \n",
    "        # # Prepare the forward task prompt\n",
    "        system_message = DA_role_prompt.value  + '\\n' + context_prompt.value + '\\n'\n",
    "        if not training_data:\n",
    "            forward_prompt_base = f\"{VM_role_text}\"\n",
    "        else:\n",
    "            forward_prompt_base = f\"////Previous Formulation and Lab Validation:\" + \"\\n\".join(training_data) + f\"\\n{VM_role_text}\"\n",
    "            # print(f'This is forward prompt {forward_prompt_base}')\n",
    "\n",
    "\n",
    "        # print(messages)\n",
    "        for idx, formulation in enumerate(unique_formulations):\n",
    "            # Create the full forward task prompt\n",
    "            forward_prompt = system_message + f\"{forward_prompt_base}\\n Considering this context, what is the compressive strength of {formulation}? Answer in this exact format: {'your estimate'} MPa\"\n",
    "            messages_forward_prediction = [{\"role\": \"user\", \"content\": forward_prompt}]    \n",
    "\n",
    "            model_response_forward_prediction = prompt_model_over_api(model_dropdown.value, messages_forward_prediction)\n",
    "\n",
    "            \n",
    "                # Extract the predicted strength from the model's response\n",
    "            try:\n",
    "                correct_output_format = re.search(r'(\\d+(\\.\\d+)?)\\s*MPa', model_response_forward_prediction)\n",
    "                if correct_output_format:\n",
    "                    predicted_strength_str = correct_output_format.group(1)  # This contains just the 'number' part\n",
    "                    predicted_strength = float(predicted_strength_str)\n",
    "                    if (predicted_strength < 25):\n",
    "                        print(f'Unusual result for following model response : \\n{model_response_forward_prediction}')\n",
    "                        continue\n",
    "                    print(f'Verifier Model forward strength prediction for {formulation} :\\n{predicted_strength}\\n')\n",
    "                else:\n",
    "                    print(f\"No correct output format for MPa found in forward model prediction {idx}. \\n {model_response_forward_prediction} \\n\")\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                print(f\"Could not convert predicted strength for {formulation} to float. Skipping this formulation.\")\n",
    "                continue\n",
    "\n",
    "            # Append to the predictions list\n",
    "            predictions.append((formulation, predicted_strength))\n",
    "        if predictions:\n",
    "            break   \n",
    "        if not predictions:\n",
    "            retry_count += 1\n",
    "            print(f\"No valid predictions were made on attempt {retry_count}. Attempting again...\")\n",
    "    \n",
    "    if not predictions:\n",
    "        print(\"No valid predictions were made. Defaulting to the first formulation.\")\n",
    "        return {'role': 'assistant', 'content': formulations[0] if formulations else 'Unable to make valid predictions.'}\n",
    "    # # Select the formulation with the highest predicted strength\n",
    "    final_response, best_strength = max(predictions, key=lambda x: x[1])\n",
    "    print(f'Formulation chosen: {final_response}')\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2398e46b-f646-4cf1-8d10-3e5f8215fa20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'number_experiments_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# experiments = 1\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# dev_cycles = 2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# iterate_prompt = \"////We have to improve the design. Ensure that your forthcoming suggestion is distinctive and hasn't been validated previously!\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# formulation_df = format_discovery_data_for_training()\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m experiment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnumber_experiments_input\u001b[49m\u001b[38;5;241m.\u001b[39mvalue):\n\u001b[1;32m     11\u001b[0m     training_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m     tested_formulations \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'number_experiments_input' is not defined"
     ]
    }
   ],
   "source": [
    "for experiment in range(number_experiments_input.value):\n",
    "    training_data = []\n",
    "    tested_formulations = []\n",
    "    print(f'Starting experiment number {experiment+1} ...\\n')\n",
    "    system_message =  DA_role_prompt.value  + '\\n' + context_prompt.value\n",
    "    highest_strength = 0.0\n",
    "\n",
    "    for dev_cycle in range(number_development_input.value):\n",
    "        print(f'Development cycle : {dev_cycle+1} \\n')\n",
    "        current_strength = 0.0\n",
    "        messages = [{\"role\": \"user\", \"content\": system_message}]\n",
    "\n",
    "        if training_data:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": \"Previously, we have tested these formulations:\\n\" + \"\\n\".join(tested_formulations)})\n",
    "                messages.append({\"role\": \"user\", \"content\": iterate_prompt.value})\n",
    "\n",
    "\n",
    "        valid_solution = False\n",
    "        while not valid_solution:\n",
    "            if test_time_dropdown.value == '3':\n",
    "                print('Extending test time...')\n",
    "                model_response = prompt_model_with_extended_test_time(model_dropdown.value, training_data, messages)\n",
    "            else:\n",
    "                model_response = prompt_model_over_api(model_dropdown.value, messages)\n",
    "            # print(\"--- Conversation History ---\")\n",
    "            # for msg in messages:\n",
    "            #     print(f\"{msg['role']}: {msg['content']} \\n\")\n",
    "    \n",
    "            # print(f'Current training data : {training_data}')\n",
    "            parsed_solution = parse_solution(model_response)\n",
    "\n",
    "            if parsed_solution is not None:\n",
    "                (lab_result, new_training_data_point) = find_matching_result(formulation_df, parsed_solution)\n",
    "                            \n",
    "                if lab_result:\n",
    "                    current_strength = lab_result\n",
    "                    if current_strength > highest_strength:\n",
    "                        highest_strength = current_strength\n",
    "                    print(f'Lab data discovered for the following formulation : {new_training_data_point}')\n",
    "                    training_data.append(f\"{new_training_data_point} resulted in a strength of {current_strength} MPa.\")\n",
    "                    tested_formulations.append(f\"{new_training_data_point}\")\n",
    "                                      \n",
    "                    valid_solution = True\n",
    "                                \n",
    "                    print(f'The suggested formulation achieved a strength of {current_strength} MPa. \\n\\n')\n",
    "                else:\n",
    "                    print(f\"Development cycle {dev_cycle+1}: No matching lab result found for suggestion {parsed_solution} \\n\\n\")\n",
    "            if not valid_solution:\n",
    "                print(f\"Development cycle {dev_cycle+1}: Model's response did not contain a valid solution. Trying again. \\n\\n\")\n",
    "                messages[-1][\"content\"] = iterate_prompt.value + \"\\nRemember the exact parameter grid: Powderkg: {360, 370, 380, 390,400, 410, 420, 430, 440, 450}, wc: {0.45, 0.5, 0.55, 0.6}, materials: {0.7/0.3, 0.6/0.4, 0.5/0.5}, curing: {ambient, heat}. It is extremly important that you stick to these values and reply in the following format: 'The formulation is Powderkg = {your estimate}, wc = {your estimate}, materials = {your estimate}, curing = {your estimate}'\"\n",
    "       \n",
    "        if minimal_target_value_input.value and current_strength >= minimal_target_value_input.value:\n",
    "            print(f\"\\nDesired compressive strength of {minimal_target_value_input.value} MPa achieved after {dev_cycle+1} iterations. The solution is {parsed_solution}. \\n\\n\")\n",
    "            break\n",
    "        \n",
    "    print(f'We have tested the following formulations : {tested_formulations} \\n\\n')\n",
    "    print(f'The highest strength out of the formulation we predicted is : {highest_strength} \\n\\n')\n",
    "\n",
    "    save_results()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
